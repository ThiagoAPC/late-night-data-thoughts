---
title: "Cache e Persist no Spark: Quando usar e quando n√£o usar"
description: "Entenda na pr√°tica quando cache ajuda, quando atrapalha, e qual estrat√©gia de persist√™ncia escolher. Spoiler: n√£o √© sempre que cachear √© a melhor ideia."
date: 2026-02-10
tags: [Spark, Python, Performance]
slug: spark-cache-persist-benchmark
authors: [thiago]
---

Uma das primeiras otimiza√ß√µes que todo mundo aprende no Spark √© o famoso `.cache()`. Mas quando realmente vale a pena usar? E qual a diferen√ßa entre `cache()` e `persist()`? Mais importante: **quando cache PIORA a performance ao inv√©s de melhorar?**

Nesse benchmark, eu rodei 4 experimentos diferentes pra responder essas perguntas de forma pr√°tica, sem dogmas. Eu acho v√°lido isso, especialmente porque no trabalho com certeza s√≥ cachear um df n√£o vai garantir performance s√≥ por j√° estar na mem√≥ria, infraestrutura tem custos, n√£o existe "sempre fa√ßa X" - existe contexto, trade-offs e decis√µes.

## Benchmark 2 - Cache e Persist: O guia pr√°tico

### Antes de rodar, vamos entender a teoria

**O que √© cache?**

Acho que a maior parte das pessoas, quando pensam em cache podem lembrar de "Ah limpa o cache do navegador, pra ver se arruma." N√£o? Ok, realmente n√£o √© um conhecimento usual pra quem n√£o est√° familiarizado com conceitos de mem√≥ria e etc, por√©m √© mais simples do que parece, na verdade, o cache √© uma camada de armazenamento tempor√°rio que guarda c√≥pias de dados acessados frequentemente para que, na pr√≥xima vez, eles sejam entregues de forma muito mais r√°pida, evitando o esfor√ßo de busc√°-los na fonte original. Quando voc√™ faz `.cache()` em um DataFrame, voc√™ s√≥ est√° dizendo pro Spark: "ei, esse dado aqui eu vou usar de novo, deixa ele na mem√≥ria RAM em vez de recalcular toda vez".

Isso √© importante pois o Spark √© **lazy** - ele n√£o executa transforma√ß√µes at√© voc√™ chamar uma **action** (como `.count()`, `.collect()`, `.show()`). Isso significa que: enquanto voc√™ escrever c√≥digos que n√£o tem rela√ß√£o com a **action** ele n√£o vai executa-la, pode parecer estranho mas na verdade isso pode ser muito √∫til, pois voc√™ pode determinar a melhor hora pra poder come√ßar a executar suas **actions** dentro de uma ETL, dentro de um plano otimizado e organizado, garantindo mais performance e velocidade, esse √© o core do **lazy evaluation** do spark, garantir que tudo esteja distribu√≠do da melhor maneira poss√≠vel pra executar as tarefas mais r√°pido.

```python
df = spark.read.parquet("dados.parquet")
df_filtrado = df.filter(col("idade") > 18)  # ‚Üê Nada acontece ainda
df_filtrado.cache()                          # ‚Üê Ainda nada!
df_filtrado.count()                          # ‚Üê AGORA sim materializa e cacheia
```

No exemplo acima veja como foi organizado o c√≥digo:

1 - Ele cria o dataframe com base no parquet.
2 - Ele define uma **Transforma√ß√£o**, algo que ele deve fazer, no caso, filtrar o df por idade > 18, e deixa anotado.
3 - Ele dita que nesse ponto √© pra guardar o que foi feito anteriormente na mem√≥ria, pra poder utilizar depois
4 - Aqui √© onde ocorre a **A√ß√£o**, na primeira vez que rodar ele n√£o vai ter o benef√≠cio do cache pois √© a primeira vez que o Spark est√° executando o plano de a√ß√£o, por√©m ele est√° contando e guardando na mem√≥ria conforme o cache() solicitou, ent√£o na pr√≥xima vez, sabemos que foi sinalizado pra ele deixar na mem√≥ria o filtro no df por idade > 18, ent√£o ele n√£o precisa fazer tudo outra vez, pois os valores filtrados j√° est√£o na mem√≥ria, dessa forma √© s√≥ o spark ir no cache e pegar o que ele guardou anteriormente.

Deu pra entender como isso melhora a performance?  O "Gargalo" (Lentid√£o) √© Ir at√© o disco, ler arquivos gigantes, converter formatos e aplicar filtros iniciais √© a parte mais cara e demorada.

O "Pulo do Gato" √© quando voc√™ d√° o .cache(), voc√™ est√° dizendo: "Spark, depois de ter todo esse trabalho de ler e filtrar, n√£o jogue isso fora! Segure na mem√≥ria RAM". As Pr√≥ximas Opera√ß√µes: Para todas as perguntas seguintes, o Spark pula a "etapa do disco" e trabalha direto com os dados que j√° est√£o mastigados na mem√≥ria RAM, que √© milhares de vezes mais r√°pida.

Em resumo: Voc√™ "sacrifica" um pouco de mem√≥ria para ganhar muito tempo. Em um job de **batch** isso √© muito recomend√°vel, porque voc√™ sabe que os dados n√£o v√£o mudar nos pr√≥ximos minutos enquanto o seu script roda para aquele batch, isso √© o que separa um c√≥digo que demora 1 hora de um que demora 5 minutos.

Tentei focar bastante nessa parte porque pra mim era confuso entender isso, e ainda mais, saber onde colocar um cache() bem posicionado. Vamos falar agora de persist()

**Cache vs Persist: qual a diferen√ßa?**

Na pr√°tica, `cache()` √© s√≥ um atalho:

```python
df.cache()  # ‚Üê  √â a mesma coisa que...
df.persist(StorageLevel.MEMORY_ONLY)  # ‚Üê ...isso aqui
```

Mas o `persist()` te d√° controle sobre **ONDE** guardar os dados, ele √© mais customiz√°vel:

| Storage Level | Onde guarda? | Vantagem | Desvantagem |
|---------------|-------------|----------|-------------|
| **MEMORY_ONLY** | S√≥ RAM | M√°xima velocidade | Se RAM encher, perde dados |
| **MEMORY_AND_DISK** | RAM com backup em disco | Seguro, quase t√£o r√°pido | Usa disco |
| **DISK_ONLY** | S√≥ disco | N√£o usa RAM | Mais lento que mem√≥ria |
| **MEMORY_ONLY_SER** | RAM serializado | Usa menos RAM | CPU pra serializar/desserializar |
| **OFF_HEAP** | Mem√≥ria fora da JVM | N√£o sofre GC pauses | Mais complexo |

**Por que isso importa?**

Imagine que voc√™ tem um pipeline de ML:

```python
df_features = prepare_features(df_raw)  # ‚Üê Transforma√ß√µes pesadas

# Treinar modelo = m√∫ltiplas passadas nos mesmos dados
for epoch in range(100):
    train_model(df_features)  # ‚Üê Sem cache, recalcula tudo 100x
```

Sem cache, o Spark recalcula `prepare_features()` **100 vezes**. Com cache, calcula **1 vez** e reutiliza.

Mas e se voc√™ vai usar os dados s√≥ **1 vez**? A√≠ cache vira overhead desnecess√°rio, que muita gente vai tentar te convencer que √© obrigat√≥rio pra ter performance mas n√£o √©.

### O Experimento

Usei o mesmo dataset que no benchmark anterior (NYC Taxi, 1.5M registros), mas dessa vez fiz transforma√ß√µes mais pesadas pra simular um cen√°rio real de feature engineering:

- C√°lculo de dist√¢ncia euclidiana entre coordenadas
- Detec√ß√£o de hor√°rio de pico
- Classifica√ß√£o de viagens longas vs curtas
- Convers√£o de dura√ß√µes

Isso cria um DataFrame "caro de calcular" - perfeito pra testar cache.

### Os 4 Experimentos Explicados

#### **Experimento 1: Baseline - M√∫ltiplas queries SEM cache**

**Prop√≥sito:** Entender o comportamento padr√£o do Spark sem otimiza√ß√µes.

**O que fiz:**
1. Carreguei os dados do Parquet
2. Apliquei transforma√ß√µes complexas (5 colunas derivadas)
3. Rodei 3 queries de agrega√ß√£o diferentes
4. **Repeti isso 3 vezes** pra ver consist√™ncia

**Por que testar isso?**
Precisava de um baseline pra comparar. Mas mais importante: queria ver se o Spark tem alguma otimiza√ß√£o interna que simula cache entre execu√ß√µes pr√≥ximas.

**Resultado esperado:** Uns 2-3 segundos por rodada, consistente.

**Resultado real:**
```
Run 1: 2.24s
Run 2: 207.42s (WTF?!)
Run 3: 0.68s
M√©dia: 70.11s
```

**O que aconteceu?**

Algumas inconsist√™ncias podem ter ocorrido aqui:
1. **Cold start**: primeira execu√ß√£o compila c√≥digo, inicializa otimiza√ß√µes (Java JIT)
2. **Sistema operacional**: cache de disco do Windows pode ter ajudado no Run 3
3. **Garbage Collection**: JVM pode ter pausado durante Run 2
4. **Spark shuffle**: pode ter tido reorganiza√ß√£o de parti√ß√µes entre runs

Isso prova que **sem cache expl√≠cito, voc√™ n√£o tem controle sobre performance, e casos como esse segundo podem acontecer**. √â loteria.

---

#### **Experimento 2: COM cache() - O speedup m√°gico**

**Prop√≥sito:** Ver o impacto real de cachear dados que s√£o usados m√∫ltiplas vezes.

**O que fiz:**
```python
df_transformed.cache()
df_transformed.count()  # ‚Üê For√ßa materializa√ß√£o
# Agora roda as mesmas 3 queries, 3 vezes
```

**Por que o `.count()` logo ap√≥s `.cache()`?**

Porque cache √© lazy! Se voc√™ n√£o for√ßar uma action, o cache s√≥ vai acontecer quando voc√™ usar o DataFrame - e a√≠ voc√™ perde a oportunidade de medir o tempo de cache separado do tempo de query.

**Resultado:**
```
Tempo pra cachear: 2.06s
Depois disso:
  Run 1: 0.30s
  Run 2: 0.55s  
  Run 3: 0.46s
  M√©dia: 0.44s
```

**Analisando os n√∫meros:**

- **70.11s ‚Üí 0.44s** = **160x mais r√°pido!**
- Mas tem o overhead inicial de 2.06s pra materializar
- Total: 2.06 + (0.44 √ó 3) = **3.38s** pra 3 rodadas
- Sem cache: 210.34s pra 3 rodadas

**Ponto de equil√≠brio:** Nesse caso, teoricamente, voc√™ precisa de pelo menos umas **2-3 queries** pro cache compensar. Se for usar o dado 1x s√≥, o overhead de 2.06s n√£o vai valer a pena.

---

#### **Experimento 3: Battle royale dos Storage Levels**

**Prop√≥sito:** Agora que vimos que o persist() √© mais customiz√°vel que o cache, vamos descobrir qual estrat√©gia de persist() √© melhor no mundo real.

**O que fiz:**
Testei 3 storage levels diferentes, cada um com uma query de agrega√ß√£o:

1. **MEMORY_ONLY**: RAM pura, sem backup
2. **MEMORY_AND_DISK**: RAM com fallback pra disco
3. **DISK_ONLY**: S√≥ disco

**Por que testar isso?**

Em produ√ß√£o, voc√™ nem sempre (quase nunca :p) tem RAM infinita. Se seu DataFrame √© gigante e voc√™ usa `MEMORY_ONLY`, o Spark vai tentar cachear tudo na RAM. Se n√£o couber, ele **descarta** parti√ß√µes antigas pra abrir espa√ßo - e a√≠ voc√™ perde o cache das primeiras parti√ß√µes quando t√° processando as √∫ltimas, isso pode gerar inconsist√™ncias e perda de dados.

`MEMORY_AND_DISK` √© mais seguro: se RAM encher, despeja pro disco. Voc√™ ainda tem o cache, s√≥ que mais lento pras parti√ß√µes que foram pro disco.

**Resultados:**

| Storage Level | Tempo de cache | Tempo de query | Total |
|---------------|----------------|----------------|-------|
| MEMORY_ONLY | 1.37s | 0.25s | 1.62s |
| MEMORY_AND_DISK | 1.23s | 0.15s | 1.38s |
| DISK_ONLY | 1.20s | 0.21s | 1.41s |

**Surpresa:** MEMORY_AND_DISK teve tempo de query **mais r√°pido!** Bom, isso significa que tudo que eu falei foi pro espa√ßo? N√£o, vamos entender:

**Por qu√™?**

Duas raz√µes:

1. **Serializa√ß√£o otimizada**: O Spark sabe que pode precisar despejar dados pro disco, ent√£o serializa de forma mais eficiente desde o in√≠cio. Com MEMORY_ONLY, ele guarda objetos Java "crus", que ocupam mais espa√ßo.

2. **Dataset pequeno**: Com apenas ~2GB em mem√≥ria, tudo coube na RAM. N√£o houve spill pra disco. Mas ganhamos a serializa√ß√£o otimizada de gra√ßa!

**Li√ß√£o:** Para datasets pequenos-m√©dios (que cabem na RAM), `MEMORY_AND_DISK` n√£o tem desvantagem pr√°tica e te d√° seguran√ßa. √â minha recomenda√ß√£o padr√£o.

---

#### **Experimento 4: O teste da query √∫nica**

**Prop√≥sito:** Responder a pergunta: "Vale cachear se vou usar os dados S√ì UMA VEZ?"

**O que fiz:**
Rodei uma √∫nica query de agrega√ß√£o de duas formas:
1. Sem cache - leitura direto do Parquet
2. Com cache - cache + query

**Resultado:**
```
Sem cache: 0.33s
Com cache: 1.12s (0.79s cache + 0.33s query)
```

**An√°lise:**

Cache adicionou **0.79s de overhead**. Para uma query √∫nica, voc√™ fica **2.4x mais lento**, ent√£o t√° a√≠ para quem fica falando que em tudo tem que colocar cache porque √© uma "boa pr√°tica".

**Por que cache √© mais lento aqui?**

Porque cache envolve:
1. Desserializar dados do Parquet
2. Aplicar transforma√ß√µes
3. Serializar e guardar na RAM
4. Desserializar da RAM pra executar a query

Sem cache, voc√™ pula o passo 3 e usa direto do Parquet (que j√° √© otimizado e comprimido).

**Quando vale cachear ent√£o?**

Vamos fazer as contas. Se o overhead √© 0.79s e cada query sem cache leva 0.33s:

```
Queries | Sem cache | Com cache | Vencedor
--------|-----------|-----------|----------
1       | 0.33s     | 1.12s     | SEM cache
2       | 0.66s     | 1.45s     | SEM cache  
3       | 0.99s     | 1.78s     | SEM cache
4       | 1.32s     | 2.11s     | SEM cache
5       | 1.65s     | 2.44s     | SEM cache
10      | 3.30s     | 4.09s     | SEM cache
20      | 6.60s     | 7.39s     | SEM cache
```

Espera... cache nunca ganha? ü§î

**N√£o!** Esse c√°lculo assume que "sem cache" sempre vai levar 0.33s. Mas lembra do Experimento 1? Sem cache √© **inconsistente**. Em produ√ß√£o, voc√™ pode ter:
- Conten√ß√£o de I/O (outros jobs lendo o mesmo storage)
- Network latency (se dados t√£o no S3)
- Cold cache do OS

E l√° no Experimento 2, vimos que sem cache a m√©dia foi **70.11s** por rodada, n√£o 0.33s.

**A verdade:** Query √∫nica em ambiente ideal = n√£o precisa de cache. Query √∫nica em produ√ß√£o com dados grandes = depende do contexto.

---

### Os Resultados - Resum√£o

| Teste | Tempo | O que aprendi |
|-------|-------|---------------|
| Sem cache (3 runs) | 70.11s/run | Inconsistente, n√£o confi√°vel |
| Com cache (3 runs) | 0.44s/run | **160x speedup** |
| Overhead de cache | 2.06s | Precisa de 3+ queries pra compensar |
| MEMORY_ONLY | 0.25s | R√°pido mas arriscado |
| MEMORY_AND_DISK | 0.15s | **Mais r√°pido** e seguro! |
| DISK_ONLY | 0.21s | OK, mas perde pra RAM |
| Query √∫nica s/ cache | 0.33s | Baseline |
| Query √∫nica c/ cache | 1.12s | 2.4x mais lento |

### O que d√° para tirar de li√ß√£o com isso?

#### **1. Cache n√£o √© gr√°tis - tem overhead**

Toda vez que voc√™ faz `.cache()`, o Spark precisa:
- Serializar os dados
- Alocar mem√≥ria
- Gerenciar o ciclo de vida do cache

Para datasets pequenos e queries simples, esse overhead pode custar mais do que recalcular.

**Na pr√°tica:**
```python
# Ruim - cache pra uso √∫nico
df.cache()
df.show(10)  # S√≥ isso? N√£o vale

# Bom - cache pra reuso
df.cache()
df.count()  # Materializa
result1 = df.filter(...).agg(...)
result2 = df.groupBy(...).agg(...)
result3 = df.join(other_df, ...)
```

#### **2. MEMORY_AND_DISK √© o sweet spot**

Achei que ia ser mais lento por causa do fallback pra disco. Mas a serializa√ß√£o otimizada compensou.

**Recomenda√ß√£o:**
```python
from pyspark import StorageLevel

# Use isso como padr√£o
df.persist(StorageLevel.MEMORY_AND_DISK)
```

S√≥ use `MEMORY_ONLY` se:
- Voc√™ TEM CERTEZA que cabe na RAM
- Pode aceitar recomputa√ß√£o se houver eviction
- Est√° otimizando os √∫ltimos 5% de performance

#### **3. Sempre force a materializa√ß√£o**

```python
# Problema: cache √© lazy
df.cache()
result = df.filter(...).collect()  # Cache acontece aqui, misturado com a query

# Solu√ß√£o: force agora
df.cache()
df.count()  # Materializa e descarta resultado
result = df.filter(...).collect()  # Agora usa cache
```

Isso te d√°:
- Controle sobre quando o overhead acontece
- M√©tricas mais limpas (separar tempo de cache vs tempo de query)
- Previsibilidade em produ√ß√£o

#### **4. Unpersist quando terminar**

```python
df.cache()
# ... usa o dataframe v√°rias vezes ...
df.unpersist()  # Libera mem√≥ria
```

Se voc√™ n√£o fizer isso, o Spark vai gerenciar automaticamente (LRU), mas voc√™ perde controle. Em pipelines longos, isso pode causar thrashing de mem√≥ria.

#### **5. O mito das m√∫ltiplas queries**

Todo mundo fala: "s√≥ cacheia se vai usar m√∫ltiplas vezes". Mas quantas vezes?

**Nesse benchmark:** Precisou de **5+ queries** pro cache compensar o overhead em ambiente ideal.

**Em produ√ß√£o:** Pode ser diferente porque:
- Dados v√™m de storage remoto (S3, GCS) - lat√™ncia de rede
- Transforma√ß√µes complexas - recomputa√ß√£o cara
- Cluster compartilhado - conten√ß√£o de recursos

**Regra pr√°tica:** Se voc√™ vai usar 3+ vezes E as transforma√ß√µes s√£o caras (joins, window functions, UDFs), cacheia.

---

### Ent√£o quando da pra usar cache/persist?

| Situa√ß√£o | Cache? | Storage Level |
|----------|--------|---------------|
| ML training (m√∫ltiplas itera√ß√µes) | SIM | MEMORY_AND_DISK |
| Explora√ß√£o interativa (Jupyter) | SIM | MEMORY_AND_DISK |
| Pipeline com reuso de dados | SIM | MEMORY_AND_DISK |
| ETL linear (ler ‚Üí transformar ‚Üí escrever) | N√ÉO | - |
| Query √∫nica em dados pequenos | N√ÉO | - |
| Query √∫nica em dados grandes/remotos | DEPENDE | MEMORY_AND_DISK |
| Dados intermedi√°rios usados 2x | DEPENDE | Me√ßa! |
| Dados intermedi√°rios usados 5x+ | SIM | MEMORY_AND_DISK |

### Quando o entrevistador te pedir exemplo real de quando usar e quando n√£o usar:

Imagine um pipeline de recomenda√ß√£o:

```python
# 1. Leitura e limpeza (transforma√ß√µes caras)
user_events = spark.read.parquet("events/")
user_features = prepare_user_features(user_events)  # ‚Üê Caro: joins, aggregations

# 2. Vamos usar user_features em 4 lugares:
user_features.persist(StorageLevel.MEMORY_AND_DISK)
user_features.count()  # Materializa

# 3. M√∫ltiplos consumos
popular_items = user_features.filter(...).groupBy(...)       # 1
user_segments = user_features.groupBy(...).agg(...)          # 2  
recommendations = user_features.join(item_embeddings, ...)   # 3
user_profile = user_features.agg(...)                        # 4

# 4. Limpeza
user_features.unpersist()
```

Sem cache, `prepare_user_features()` rodaria **4 vezes**. Com cache, roda **1 vez**.

### Um exemplo real de quando N√ÉO usar

```python
# ETL simples
df = spark.read.parquet("raw/")
df_clean = df.filter(col("status") == "valid")
df_clean.cache()  # Por qu√™?? Vai usar 1x s√≥!
df_clean.write.parquet("clean/")
```

Aqui o cache s√≥ adiciona overhead. Melhor:

```python
df = spark.read.parquet("raw/")
df.filter(col("status") == "valid").write.parquet("clean/")  # Simples e direto
```

### Conclus√£o

Cache n√£o √© bala de prata. √â uma ferramenta que:
- **Ajuda muito** quando voc√™ reutiliza dados caros de calcular
- **Atrapalha** quando voc√™ tenta optimizar queries √∫nicas
- **Funciona melhor** com MEMORY_AND_DISK como padr√£o
- **Precisa ser medido** no seu contexto espec√≠fico

A regra de ouro: **me√ßa antes de otimizar**. Rode seu pipeline sem cache, veja onde tem recomputa√ß√£o, a√≠ sim adicione cache estrategicamente.

No pr√≥ximo post vou explorar particionamento e shuffle - outra √°rea cheia de mitos e decis√µes ruins. At√© l√°!

---
title: "Benchmarks √∫teis do Spark, por que voc√™ deve saber disso?"
description: "Atrav√©s destes benchmarks simples do spark eu pretendo ilustrar conceitos relativamente b√°sicos sobre estrat√©gias de leitura de dados, por√©m que v√£o te auxiliar a enxergar a arquitetura de processamento de dados com um olhar um pouco mais pragm√°tico."
date: 2026-02-09
tags: [spark, python, performance, bigdata, data-engineering]
slug: benchmarks-spark
authors: [thiago]
---

Basicamente um dos pontos iniciais na carreira de todo engenheiro de dados √© conhecer, entender, experimentar e dominar frameworks de processamento de dados para Big Data, isso por que as necessidades de dados das grandes empresas aumentam exponencialmente conforme seus neg√≥cios crescem, o foco aqui n√£o √© te ensinar do b√°sico o que √© Spark e pra que ele serve, eu quero apenas ilustrar uma s√©rie de cen√°rios onde certas pr√°ticas fazem diferen√ßa na hora de utilizar o Spark e talvez me incentivar a ter mais cuidado na hora de planejar como eu farei uso dessa tecnologia, tendo um olhar mais cr√≠tico sobre o processo inteiro como um todo.

## Benchmark 1 - CSV vs Parquet, na pr√°tica o que muda?

Essa primeira ideia de benchmark me veio na cabe√ßa por que uma vez em uma entrevista me perguntaram qual a diferen√ßa entre ambos, e POR QU√ä eles s√£o diferentes, eu respondi mecanicamente que Parquet era um formato de armazenamento colunar mas n√£o tinha um conhecimento mais aprofundado pra dizer o que seria isso e como impacta em uma ETL em rela√ß√£o ao CSV (acho que eu me queimei j√° nessa pergunta :p), por√©m quero fixar isso na cabe√ßa ent√£o vamos l√°:

### Antes de rodar, vamos entender a teoria

**CSV (Comma-Separated Values):**
- Formato **orientado a linhas** - cada linha cont√©m todos os campos do registro
- Texto puro, leg√≠vel por humanos
- N√£o guarda informa√ß√£o de tipos (tudo √© string)
- Para ler uma coluna espec√≠fica, precisa ler a linha inteira

**Parquet:**
- Formato **orientado a colunas** - dados da mesma coluna ficam juntos (√© estranho pensar nisso a princ√≠pio por√©m tente enxergar como o ato de "tombar" a tabela)
- Bin√°rio, comprimido, n√£o leg√≠vel diretamente
- Guarda metadados com tipos, estat√≠sticas, etc.
- Para ler uma coluna, l√™ s√≥ aquela coluna (projection pruning)

Imagina uma tabela assim:

```
| id | nome   | idade |
|----|--------|-------|
| 1  | Ana    | 25    |
| 2  | Bruno  | 30    |
| 3  | Carla  | 28    |
```

No **CSV**, os dados ficam assim no disco:
```
1,Ana,25
2,Bruno,30
3,Carla,28
```

No **Parquet**, conceptualmente:
```
[1, 2, 3]           # coluna id
[Ana, Bruno, Carla] # coluna nome
[25, 30, 28]        # coluna idade
```

Isso muda TUDO quando voc√™ quer fazer uma query tipo `SELECT avg(idade) FROM tabela`. Porque pensa bem, voc√™ direciona a engine do Spark a ler DIRETAMENTE o que voc√™ precisa (idade, nesse caso), sem precisar varrer todas as linhas, ou seja, ao inv√©s de ler 1 milh√£o de linhas s√£o apenas as que cont√©m as colunas desejadas. 

### E a consist√™ncia dos dados?

Esse √© um ponto que muita gente ignora mas que podem te perguntar na entrevista, na hora de listar pontos positivos e negativos entre os dois, e a resposta mais exata √©: **CSV n√£o garante consist√™ncia de tipos**.

No CSV, tudo √© texto. Uma coluna "idade" pode ter:
```
25
30
N/A
vinte e oito
-1
```

O Spark s√≥ vai descobrir esse problema quando tentar converter pra n√∫mero - e a√≠ voc√™ tem nulls inesperados, erros silenciosos ou jobs que quebram no meio, gerando dor de cabe√ßa.

J√° o **Parquet tem schema embutido**. Os tipos s√£o definidos na escrita e validados na leitura. Se voc√™ tenta escrever uma string numa coluna INTEGER, o erro acontece antes, na escrita - n√£o depois, quando o BI j√° t√° mostrando dados errados pro neg√≥cio.

Isso √© especialmente relevante em pipelines de dados onde:
- Dados v√™m de fontes externas (APIs, parceiros, uploads manuais)
- M√∫ltiplos times escrevem no mesmo dataset
- Voc√™ precisa garantir qualidade de dados pra downstream (BI, ML)

A [documenta√ß√£o oficial do Parquet](https://parquet.apache.org/docs/file-format/) explica como o schema √© armazenado nos metadados do arquivo, garantindo que leituras futuras respeitem os tipos originais.

### Um experimento pr√°tico sobre o assunto

Pra ter um exemplo mais interessante e aplic√°vel em situa√ß√£o de neg√≥cios, eu usei o dataset **NYC Taxi Trip Duration** do Kaggle (~1.5 milh√µes de viagens, 192MB em CSV) e rodei os seguintes testes:

1. Ler CSV com schema **inferido** (Spark adivinha os tipos)
2. Ler CSV com schema **definido** (eu passo os tipos)
3. Converter para Parquet e ler
4. Fazer queries simples em ambos

### Os Resultados

Aqui √© onde a coisa fica interessante:

| Teste | Tempo | Observa√ß√£o |
|-------|-------|------------|
| CSV - Schema Inferido | **4.71s** | üò± Spark leu o arquivo 2x |
| CSV - Schema Definido | **0.60s** | 8x mais r√°pido! |
| Parquet - Leitura | **0.55s** | Ainda mais r√°pido |
| Parquet - Query agrega√ß√£o | **0.31s** | 2x mais r√°pido que CSV |
| Parquet - Query com filtro | **0.34s** | Predicate pushdown |
| Parquet - Lendo 2 de 11 colunas | **0.42s** | Projection pruning |

E o tamanho em disco?

```
CSV:     191.30 MB
Parquet:  57.93 MB  ‚Üí 70% menor!
```

### O que da pra tirar de li√ß√£o com isso?

**1. Schema inferido: cilada ou praticidade**

Quando voc√™ faz `spark.read.csv("arquivo.csv", inferSchema=True)`, o Spark precisa:
1. Ler o arquivo inteiro uma vez s√≥ pra descobrir os tipos (se √© string, double, int e etc)
2. Ler de novo pra realmente carregar os dados

S√£o 4.71s vs 0.60s - quase **8x de diferen√ßa**. Em produ√ß√£o com TBs de dados, isso √© a diferen√ßa entre uma pipeline de 1 hora e uma de 8 horas. Ou seja custos de 1 hora vs custos de 8 horas, deu pra entender como isso d√≥i no bolso e gera menos valor ao cliente.

Mas calma - isso n√£o significa que inferir √© sempre ruim. A decis√£o depende do contexto:

**Quando vale a pena inferir:**
- Explora√ß√£o inicial de dados (voc√™ ainda n√£o conhece o schema)
- Datasets pequenos onde o overhead √© irrelevante
- Prototipagem r√°pida e one-offs
- Quando o schema muda frequentemente e voc√™ quer flexibilidade pra se adaptar √†s mudan√ßas

**Quando N√ÉO vale:**
- Pipelines de produ√ß√£o que rodam recorrentemente
- Datasets gigantes (GBs/TBs)
- Quando voc√™ j√° conhece a regra de neg√≥cio e o schema √© est√°vel
- Quando custo de cloud √© uma preocupa√ß√£o

**Li√ß√£o:** Avalie o trade-off. Definir schema expl√≠cito deixa c√≥digo mais verboso e "acoplado", mas em produ√ß√£o com dados grandes, o ganho de performance compensa. Pra explora√ß√£o e prototipagem, infer d√° pra usar tranquilo.

```python
# Em produ√ß√£o com schema est√°vel:
df = spark.read.schema(schema).csv("arquivo.csv", header=True)

# Em explora√ß√£o ou dados pequenos:
df = spark.read.csv("arquivo.csv", header=True, inferSchema=True)
```

**2. Parquet comprime absurdamente bem**

70% de redu√ß√£o n√£o √© coincid√™ncia. Como os dados da mesma coluna ficam juntos, a compress√£o funciona muito melhor. Uma coluna de `vendor_id` que s√≥ tem valores 1 e 2 comprime pra quase nada. No CSV, esses valores est√£o espalhados em linhas e mais linhas dizendo pouca coisa v√°rias vezes diferentes.

No nosso exemplo, tivemos o seguinte resultado:

Convertendo CSV para Parquet...
----------------------------------------
  Tempo convers√£o: 4.74s
  Tamanho CSV: 191.30 MB
  Tamanho Parquet: 57.93 MB
  Compress√£o: 69.7% menor

Repare o tamanho do arquivo antes e depois da convers√£o, voc√™ economiza espa√ßo e ganha mais performance!

**Li√ß√£o:** Se poss√≠vel converta seus dados para Parquet na primeira etapa do pipeline.

**3. Predicate Pushdown √© m√°gica**

Quando fiz uma query com filtro (`passenger_count > 2 AND trip_duration > 600`), o Parquet n√£o precisou ler todos os 1.4 milh√µes de registros no dataset. Ele usa as estat√≠sticas armazenadas nos metadados (min/max de cada bloco) para pular blocos inteiros que n√£o atendem o filtro.

Teste adicional: Query com filtro (predicate pushdown)
  Linhas filtradas: 121,168
  Tempo: 0.34s

**4. Projection Pruning em a√ß√£o**

Quando li s√≥ 2 colunas de 11:
- CSV: precisa ler a linha inteira, depois descartar 9 colunas
- Parquet: l√™ fisicamente s√≥ as 2 colunas necess√°rias

### Ent√£o quando da pra usar cada um?

| Situa√ß√£o | Use |
|----------|-----|
| Dados de entrada de sistemas legados | CSV (√© o que voc√™ provavelmente vai receber pra ETL) |
| Troca com ferramentas que n√£o suportam Parquet | CSV |
| Armazenamento intermedi√°rio/final | **Parquet** |
| Dados que ser√£o lidos v√°rias vezes | **Parquet** |
| Queries anal√≠ticas (BI, relat√≥rios) | **Parquet** |

### Conclus√£o

O takeaway principal √©: **CSV √© pra entrada, Parquet √© pra processamento**.

Se voc√™ t√° lendo CSV direto do seu data lake em cada etapa do pipeline, voc√™ t√° literalmente jogando performance (e dinheiro de cloud) no lixo. Converta pra Parquet logo na ingest√£o e colha os benef√≠cios em todas as etapas seguintes.

Nos pr√≥ximos posts vou explorar outros benchmarks: cache/persist, particionamento, estrat√©gias de join e AQE. Valeu!

